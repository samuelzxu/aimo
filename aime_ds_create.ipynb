{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from the llm_evals table at this psql link\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "conn_str = os.getenv('PG_STR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = psycopg2.connect(conn_str)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT * FROM llm_evals\")\n",
    "\n",
    "results = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [i[0] for i in cursor.description]\n",
    "\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prop_correct\"] = df.apply(lambda x: x.extracted_answers.count(x.label)/(len(x.extracted_answers) or 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"run_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run = df[df[\"run_name\"] == 'DeepSeek-R1-Distill-Qwen-14B-AWQ-5origprompt-H100-16384-t0.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run[\"prop_correct\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run_easy_medium = aime_run[aime_run[\"prop_correct\"].apply(lambda x: x < 0.85 and x>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run_hard = aime_run[aime_run[\"prop_correct\"].apply(lambda x: x == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def my_func(x):\n",
    "    x = literal_eval(x)\n",
    "    if x[0][1][\"role\"] != \"user\":\n",
    "        raise TypeError()\n",
    "    return x[0][1][\"content\"]\n",
    "\n",
    "aime_run_easy_medium[\"problem\"] = aime_run_easy_medium[\"reasoning\"].copy(deep=True).apply(my_func)\n",
    "aime_run_hard[\"problem\"] = aime_run_hard[\"reasoning\"].copy(deep=True).apply(my_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run_easy_medium[\"solution\"] = aime_run_easy_medium[\"label\"].apply(lambda x: f\"\\\\boxed{{{int(x)}}}\")\n",
    "aime_run_hard[\"solution\"] = aime_run_hard[\"label\"].apply(lambda x: f\"\\\\boxed{{{int(x)}}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many of the hard and easy_medium problems have [asy] tags in them\n",
    "# Count problems with [asy] tags\n",
    "asy_count_easy_medium = aime_run_easy_medium[\"problem\"].str.contains(\"\\[asy\\]\").sum()\n",
    "asy_count_hard = aime_run_hard[\"problem\"].str.contains(\"\\[asy\\]\").sum()\n",
    "\n",
    "print(f\"Problems with [asy] tags in easy/medium set: {asy_count_easy_medium}\")\n",
    "print(f\"Problems with [asy] tags in hard set: {asy_count_hard}\")\n",
    "\n",
    "# Remove these problems\n",
    "aime_run_easy_medium = aime_run_easy_medium[~aime_run_easy_medium[\"problem\"].str.contains(\"\\[asy\\]\")]\n",
    "aime_run_hard = aime_run_hard[~aime_run_hard[\"problem\"].str.contains(\"\\[asy\\]\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run_easy_medium = aime_run_easy_medium.drop(columns=['uuid', 'exec_time', 'runtime_s', 'p_id', 'run_name', 'prediction',\n",
    "       'label', 'extracted_answers', 'reasoning', 'prop_correct'])\n",
    "aime_run_hard = aime_run_hard.drop(columns=['uuid', 'exec_time', 'runtime_s', 'p_id', 'run_name', 'prediction',\n",
    "       'label', 'extracted_answers', 'reasoning', 'prop_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize tinputs and find their max size using the qwen tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    "\n",
    "# Combine easy_medium and hard datasets\n",
    "aime_run = pd.concat([aime_run_easy_medium, aime_run_hard])\n",
    "\n",
    "# Tokenize problems and solutions\n",
    "problem_tokens = aime_run[\"problem\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "solution_tokens = aime_run[\"solution\"].apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "print(f\"Max problem tokens: {problem_tokens.max()}\")\n",
    "print(f\"Mean problem tokens: {problem_tokens.mean():.1f}\")\n",
    "print(f\"Max solution tokens: {solution_tokens.max()}\")\n",
    "print(f\"Mean solution tokens: {solution_tokens.mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aime_run[\"problem\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all aime run questions end with \"modulo 1000.\"\n",
    "sum(aime_run[\"problem\"].apply(lambda text: text[-13:] == \"modulo 1000. \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to huggingface dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "hf_tok = os.getenv(\"HF_TOK\")\n",
    "\n",
    "# Convert pandas DataFrame to Huggingface Dataset\n",
    "hf_dataset = Dataset.from_pandas(aime_run_easy_medium.iloc[:10], split=\"train\")\n",
    "\n",
    "# Push to hub\n",
    "hf_dataset.push_to_hub(\n",
    "    \"samitizerxu/math-easy-medium-t\",\n",
    "    private=True,\n",
    "    token=hf_tok  # Replace with actual token\n",
    ")\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(aime_run_hard.iloc[:10], split=\"train\")\n",
    "\n",
    "# Push to hub\n",
    "hf_dataset.push_to_hub(\n",
    "    \"samitizerxu/math-hard-t\",\n",
    "    private=True,\n",
    "    token=hf_tok  # Replace with actual token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = Dataset.from_pandas(aime_run.iloc[:10], split=\"train\")\n",
    "\n",
    "# Push to hub\n",
    "hf_dataset.push_to_hub(\n",
    "    \"samitizerxu/math-all-t\",\n",
    "    private=True,\n",
    "    token=hf_tok  # Replace with actual token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
