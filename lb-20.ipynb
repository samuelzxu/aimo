{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 1.8em; color: #2c3e50;\">Optimizing the Public Notebook</h1>\n",
    "\n",
    "<p style=\"font-size: 1.1em; color: #444;\">\n",
    "When I looked at the public notebook scoring <strong>16</strong>, I realized it could be improved by carefully optimizing the inference. Here's the changes I made to the public notebook and why  : </p>\n",
    "\n",
    "<h2 style=\"color: #34495e;\">1. <strong>dtype and max_num_steps</strong></h2>\n",
    "<p style=\"color: #444;\">\n",
    "Before the QwQ phase, I had a score of <strong>8</strong> with Qwen-2.5-Math-7B. I noticed that setting <code>dtype = bfloat16</code> and increasing <code>max_num_seqs</code> improved the score and made it more stable.  \n",
    "</p>\n",
    "<blockquote style=\"background: #f4f4f9; padding: 10px; border-left: 4px solid #a3a3a3; margin: 10px 0;\">\n",
    "<strong>Settings:</strong>  \n",
    "<code>dtype = bfloat16</code>, <code>max_num_seqs = 256 (default)</code>\n",
    "</blockquote>\n",
    "\n",
    "<h2 style=\"color: #34495e;\">2. <strong>Change in Prompts</strong></h2>\n",
    "<p style=\"color: #444;\">\n",
    "Since QwQ leverages Chain-of-Thought (CoT) better than code generation, I decided to retain only the CoT prompts and remove those mentioning Python code generation.  \n",
    "Here are the 5 prompts I used for each sample:\n",
    "</p>\n",
    "<blockquote style=\"background: #f4f4f9; padding: 10px; border-left: 4px solid #a3a3a3; margin: 10px 0;\">\n",
    "<ul>\n",
    "  <li>Please use chained reasoning to put the answer in <code>\\\\boxed{}</code>.</li>\n",
    "  <li>Please reflect and verify while reasoning and put the answer in <code>\\\\boxed{}</code>.</li>\n",
    "  <li>Solve the following problem using concise and clear reasoning by placing the answer in <code>\\\\boxed{}</code>.</li>\n",
    "  <li>You are a helpful and reflective maths assistant, please reason step by step to put the answer in <code>\\\\boxed{}</code>.</li>\n",
    "  <li>You are the smartest maths expert in the world, please spike this question and put the answer in <code>\\\\boxed{}</code>.</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "<h2 style=\"color: #34495e;\">3. <strong>Number of Samples: 8 ➔ 5</strong></h2>\n",
    "<p style=\"color: #444;\">\n",
    "From my local runs and discussions, I observed that QwQ mostly provided the correct answer with fewer samples (4–5). Using 8 samples was inefficient, as it increased runtime and risked exceeding the 5-hour limit before solving all 50 problems (solving 40-45 in total). Therefore, I reduced the number of samples to 5.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T16:08:51.375756Z",
     "iopub.status.busy": "2024-12-15T16:08:51.375460Z",
     "iopub.status.idle": "2024-12-15T16:09:20.605222Z",
     "shell.execute_reply": "2024-12-15T16:09:20.604419Z",
     "shell.execute_reply.started": "2024-12-15T16:08:51.375713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#fixed seed to get similar score\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.33725Z",
     "iopub.status.idle": "2024-12-03T18:18:05.337569Z",
     "shell.execute_reply": "2024-12-03T18:18:05.337429Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.337408Z"
    },
    "papermill": {
     "duration": 15.076705,
     "end_time": "2024-10-27T05:59:33.712437",
     "exception": false,
     "start_time": "2024-10-27T05:59:18.635732",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cutoff_time = time.time() + (4 * 60 + 45) * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.338367Z",
     "iopub.status.idle": "2024-12-03T18:18:05.338666Z",
     "shell.execute_reply": "2024-12-03T18:18:05.338534Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.338518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def clean_memory(deep=False):\n",
    "    gc.collect()\n",
    "    if deep:\n",
    "        ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "llm_model_pth = '/home/ziggy/models/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    #dtype=\"half\",                -> Changed this\n",
    "    #max_num_seqs=128,            -> Changed this\n",
    "    max_model_len=32768,#4096*10,         \n",
    "    trust_remote_code=True,     \n",
    "    tensor_parallel_size=1,      \n",
    "    gpu_memory_utilization=0.96, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.339809Z",
     "iopub.status.idle": "2024-12-03T18:18:05.340152Z",
     "shell.execute_reply": "2024-12-03T18:18:05.34001Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.339994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utlities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.341335Z",
     "iopub.status.idle": "2024-12-03T18:18:05.341642Z",
     "shell.execute_reply": "2024-12-03T18:18:05.341507Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.341491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#prompts\n",
    "thoughts = [\n",
    "   \n",
    "    'Please use chained reasoning to put the answer in \\\\boxed{}.',\n",
    "    'Please reflect and verify while reasoning and put the answer in \\\\boxed{}.',\n",
    "    'Solve the following problem using concise and clear reasoning by placing the answer in \\\\boxed{}.',\n",
    "    'You are a helpful and reflective maths assistant, please reason step by step to put the answer in \\\\boxed{}.',\n",
    "    'You are the smartest maths expert in the world, please spike this question and put the answer in \\\\boxed{}.'\n",
    "]\n",
    "\n",
    "#create single prompt\n",
    "def make_next_prompt(text,round_idx):\n",
    "    default_prompt = thoughts[(round_idx+1)%len(thoughts)]\n",
    "    default_python_code = f\"print('{default_prompt}')\"\n",
    "    return default_python_code\n",
    "\n",
    "#extract python code from response\n",
    "def extract_python_code(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if matches:\n",
    "        ans = \"\\n\\n\".join(matches)\n",
    "        #print(f'Extracted python code: {ans}')\n",
    "        return ans\n",
    "    return \"\"\n",
    "\n",
    "#extract all code segments\n",
    "def extract_python_code_list(text):\n",
    "    pattern = r'```python\\s*(.*?)\\s*```'\n",
    "    ans=[]\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    for m in matches:\n",
    "        ans.append(m)\n",
    "    return ans\n",
    "\n",
    "#process the code\n",
    "def process_python_code(query):\n",
    "    query = \"import math\\nimport numpy as np\\nimport sympy as sp\\n\" + query\n",
    "    current_rows = query.strip().split(\"\\n\")\n",
    "    new_rows = []\n",
    "    for row in current_rows:\n",
    "        new_rows.append(row)\n",
    "    ans = \"\\n\".join(new_rows)\n",
    "    print(f'Processed python code: {ans}')\n",
    "    return ans\n",
    "\n",
    "import re\n",
    "\n",
    "#extract the answer from the boxes\n",
    "def extract_boxed_texts(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return []\n",
    "    ans = []\n",
    "    for content in matches:\n",
    "        if content.isdigit():\n",
    "            num = int(content)\n",
    "        else:\n",
    "            nums = re.findall(r'\\d+', content)\n",
    "            if not nums:\n",
    "                continue\n",
    "            num = int(nums[-1])\n",
    "        ans.append(num % 1000)\n",
    "    return ans\n",
    "    \n",
    "#extract the integer answer modulo 1000 from the boxes\n",
    "def extract_boxed_text(text):\n",
    "    pattern = r'oxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return -1\n",
    "    content = matches[0]\n",
    "    if content.isdigit():\n",
    "        num = int(content)\n",
    "    else:\n",
    "        nums = re.findall(r'\\d+', content)\n",
    "        if not nums:\n",
    "            return -1\n",
    "        num = int(nums[-1])\n",
    "    return num % 1000\n",
    "\n",
    "#select the final answer based on the frequency (majoity voting)\n",
    "from collections import Counter\n",
    "def select_answer(answers):\n",
    "    valid_answers = []\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            if int(answer) == float(answer):\n",
    "                if 1 < int(answer) < 999 and int(answer) % 100 > 0:\n",
    "                    valid_answers.append(int(answer))\n",
    "        except:\n",
    "            pass\n",
    "    if not valid_answers:\n",
    "        return 49\n",
    "    _, answer = sorted([(v,k) for k,v in Counter(valid_answers).items()], reverse=True)[0]\n",
    "    return answer%1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.342516Z",
     "iopub.status.idle": "2024-12-03T18:18:05.342829Z",
     "shell.execute_reply": "2024-12-03T18:18:05.342696Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.34268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "#Python REPL to execute code. taken and modified from NuminaMath Solution\n",
    "#NuminaMath solution can be found here : https://www.kaggle.com/code/lewtun/numina-1st-place-solution\n",
    "\n",
    "class PythonREPL:\n",
    "    def __init__(self, timeout=8):\n",
    "        self.timeout = timeout\n",
    "\n",
    "    def __call__(self, query):\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            temp_file_path = os.path.join(temp_dir, \"tmp.py\")\n",
    "            with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(query)\n",
    "            \n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [\"python3\", temp_file_path],\n",
    "                    capture_output=True,\n",
    "                    check=False,\n",
    "                    text=True,\n",
    "                    timeout=self.timeout,\n",
    "                )\n",
    "            except subprocess.TimeoutExpired:\n",
    "                return False, f\"Execution timed out after {self.timeout} seconds.\"\n",
    "\n",
    "            stdout = result.stdout.strip()\n",
    "            stderr = result.stderr.strip()\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                return True, stdout\n",
    "            else:\n",
    "                # Process the error message to remove the temporary file path\n",
    "                # This makes the error message cleaner and more user-friendly\n",
    "                error_lines = stderr.split(\"\\n\")\n",
    "                cleaned_errors = []\n",
    "                for line in error_lines:\n",
    "                    if temp_file_path in line:\n",
    "                        # Remove the path from the error line\n",
    "                        line = line.replace(temp_file_path, \"<temporary_file>\")\n",
    "                    cleaned_errors.append(line)\n",
    "                cleaned_error_msg = \"\\n\".join(cleaned_errors)\n",
    "                # Include stdout in the error case\n",
    "                combined_output = f\"{stdout}\\n{cleaned_error_msg}\" if stdout else cleaned_error_msg\n",
    "                return False, combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.344049Z",
     "iopub.status.idle": "2024-12-03T18:18:05.344355Z",
     "shell.execute_reply": "2024-12-03T18:18:05.344218Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.344202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "list_of_texts = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for messages in [[{\"role\": \"user\", \"content\": \"hi\"}]]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.345289Z",
     "iopub.status.idle": "2024-12-03T18:18:05.34559Z",
     "shell.execute_reply": "2024-12-03T18:18:05.345456Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.345441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#define the sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,              # Controls randomness in generation: higher values (e.g., 1.0) produce more diverse output.\n",
    "    min_p=0.01,                   # Minimum cumulative probability for nucleus sampling, filtering out unlikely tokens.\n",
    "    skip_special_tokens=True,     \n",
    "    # max_tokens=1800,            \n",
    "    max_tokens=32768,             # Sets a very high limit for token generation to handle longer outputs.\n",
    "    # stop=[\"```output\"],       \n",
    ")\n",
    "\n",
    "#generate prompts in batch\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    list_of_texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            conversation=messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        for messages in list_of_messages\n",
    "    ]\n",
    "    \n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "        print(messages[-1])\n",
    "\n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.3466Z",
     "iopub.status.idle": "2024-12-03T18:18:05.346929Z",
     "shell.execute_reply": "2024-12-03T18:18:05.346768Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.346753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#filter answers from the responses\n",
    "\n",
    "def batch_message_filter(list_of_messages,list_of_idx) -> tuple[list[list[dict]], list[str]]:\n",
    "    global answer_contributions\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    list_of_idx_to_keep = []\n",
    "    for idx,messages in zip(list_of_idx,list_of_messages):\n",
    "        answers = extract_boxed_texts(messages[-1]['content'])\n",
    "        if answers:\n",
    "            extracted_answers.extend(answers)\n",
    "            for answer in answers:\n",
    "                answer_contributions[answer].append(idx)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "            list_of_idx_to_keep.append(idx)\n",
    "    return list_of_messages_to_keep, extracted_answers, list_of_idx_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.348195Z",
     "iopub.status.idle": "2024-12-03T18:18:05.348508Z",
     "shell.execute_reply": "2024-12-03T18:18:05.348369Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.348354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#execute the codes in the responses\n",
    "def batch_message_execute(list_of_messages,round_idx) -> list[list[dict]]:\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'],round_idx)\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        messages.append({'role': 'user', 'content': output})\n",
    "        print(messages[-1])\n",
    "    return list_of_messages\n",
    "\n",
    "#execute the code and generate the answer from responses\n",
    "def batch_message_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code = extract_python_code(messages[-1]['content'])\n",
    "        python_code = process_python_code(python_code)\n",
    "        try:\n",
    "            success, output = PythonREPL()(python_code)\n",
    "            if success:\n",
    "                patten = r'(\\d+)'\n",
    "                matches = re.findall(patten, output)\n",
    "                if matches:\n",
    "                    for match in matches:\n",
    "                        ans.append(int(match)%1000)\n",
    "                        ans.append(int(match)%1000) #代码权重高于自然语言，所以添加两次 \n",
    "        except Exception as e:\n",
    "            output = str(e)\n",
    "        print(f'python code output: {output}')\n",
    "    return ans\n",
    "\n",
    "#execute code and generate answer for all elements in batch\n",
    "def batch_message_list_execute_and_get_answer(list_of_messages,round_idx) -> tuple[list[list[dict]], list[int]]:\n",
    "    ans = []\n",
    "    for messages in list_of_messages:\n",
    "        python_code_list = extract_python_code_list(messages[-1]['content'])\n",
    "        for python_code in python_code_list:\n",
    "            python_code = process_python_code(python_code)\n",
    "            try:\n",
    "                success, output = PythonREPL()(python_code)\n",
    "                if success:\n",
    "                    patten = r'(\\d+)'\n",
    "                    matches = re.findall(patten, output)\n",
    "                    if matches:\n",
    "                        for match in matches:\n",
    "                            ans.append(int(match)%1000)\n",
    "                            ans.append(int(match)%1000) \n",
    "            except Exception as e:\n",
    "                output = str(e)\n",
    "            print(f'python code output: {output}')\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.349366Z",
     "iopub.status.idle": "2024-12-03T18:18:05.349668Z",
     "shell.execute_reply": "2024-12-03T18:18:05.349534Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.349518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "#API for competition submission\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.350621Z",
     "iopub.status.idle": "2024-12-03T18:18:05.350952Z",
     "shell.execute_reply": "2024-12-03T18:18:05.350787Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.350772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#correct answers for reference problems\n",
    "def get_correct_answer(question):\n",
    "    if 'Three airline' in question: return 79\n",
    "    if 'Fred and George' in question: return 250\n",
    "    if 'Triangle $ABC$' in question: return 180\n",
    "    if 'Find the three' in question: return 143\n",
    "    if 'We call a' in question: return 3\n",
    "    if 'Let $ABC$ be' in question: return 751\n",
    "    if 'For a positive' in question: return 891\n",
    "    if 'For positive integers' in question: return 810\n",
    "    if 'The Fibonacci numbers' in question: return 201\n",
    "    if 'Alice writes all' in question: return 902\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.352104Z",
     "iopub.status.idle": "2024-12-03T18:18:05.35239Z",
     "shell.execute_reply": "2024-12-03T18:18:05.352261Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.352247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#predict function to solve single problem\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "g_score = 0\n",
    "g_count = 0\n",
    "prompt_score = Counter()\n",
    "answer_contributions = defaultdict(list)\n",
    "def predict_for_question(question: str) -> int:\n",
    "    global g_score\n",
    "    global g_count\n",
    "    global prompt_score\n",
    "    global answer_contributions\n",
    "    question += \"\\nIf the final answer is a number larger than 1000, take modulo 1000. \"\n",
    "    if time.time() > cutoff_time: \n",
    "        return 210\n",
    "    print(question)\n",
    "    \n",
    "    list_of_messages = [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": thoughts[k]},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ] for k in range(5)\n",
    "    ]\n",
    "\n",
    "    all_extracted_answers = []\n",
    "    list_of_idx = list(range(len(list_of_messages)))\n",
    "    max_round = 1\n",
    "    for round_idx in range(max_round):\n",
    "        print(f\"round {round_idx+1}\")\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        extracted_python_answer = batch_message_list_execute_and_get_answer(list_of_messages,round_idx)\n",
    "        list_of_messages, extracted_answers, list_of_idx  = batch_message_filter(list_of_messages, list_of_idx)\n",
    "        all_extracted_answers.extend(extracted_python_answer)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "        print(\"extracted boxed answers:\",extracted_answers)\n",
    "        print(\"extracted python answers:\",extracted_python_answer)\n",
    "        print(\"all extracted answers:\",all_extracted_answers)\n",
    "        if not list_of_messages:\n",
    "            break\n",
    "    answer = select_answer(all_extracted_answers)\n",
    "    print(\"answer:\",answer)\n",
    "    correct_answer = get_correct_answer(question)\n",
    "    print(\"correct answer:\",correct_answer)\n",
    "    g_count += 1\n",
    "    if str(answer) == str(correct_answer):\n",
    "        g_score += 1\n",
    "\n",
    "    print(f\"score: {g_score}/{g_count}\")\n",
    "    print(\"\\n\\n\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict Function provided by the hosts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.353384Z",
     "iopub.status.idle": "2024-12-03T18:18:05.353666Z",
     "shell.execute_reply": "2024-12-03T18:18:05.353539Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.353525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    print(question)\n",
    "    print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and save the reference set to use for validation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.354753Z",
     "iopub.status.idle": "2024-12-03T18:18:05.355084Z",
     "shell.execute_reply": "2024-12-03T18:18:05.354937Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.354921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-03T18:18:05.355951Z",
     "iopub.status.idle": "2024-12-03T18:18:05.356261Z",
     "shell.execute_reply": "2024-12-03T18:18:05.356127Z",
     "shell.execute_reply.started": "2024-12-03T18:18:05.356111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            'reference.csv',\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References of previous notebooks**\n",
    "\n",
    "- https://www.kaggle.com/code/richolson/ai-math-olympiad-qwen2-5-72b - shows how to submit using 72B models\n",
    "- https://www.kaggle.com/code/abdullahmeda/load-72b-awq-model-using-vllm-on-l4-x4 - shows how to use 72B models with vllm on L4x4 GPU's\n",
    "- https://www.kaggle.com/code/huikang/qwen2-5-math-1-5b-instruct - created the initial baseline, the skeleton of this notebook is inspired by this one\n",
    "- https://www.kaggle.com/code/boristown/qwen-qwq-32b-preview-deepreasoning - showed that QwQ preview is able to get high score\n",
    "\n",
    "**QwQ-32B-preview**\n",
    "\n",
    "- [Huggingface release](Qwen/QwQ-32B-Preview) - Original Huggingface Release of QwQ model by Qwen\n",
    "- [AWQ Version](https://huggingface.co/KirillR/QwQ-32B-Preview-AWQ) - QwQ AWQ Quantized Version on huggingface\n",
    "- [Blog post](https://qwenlm.github.io/blog/qwq-32b-preview/) - Detailed blog post about QwQ model\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9869096,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "sourceId": 205183965,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 127417,
     "modelInstanceId": 118183,
     "sourceId": 139552,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 176602,
     "modelInstanceId": 154124,
     "sourceId": 180858,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 172131,
     "modelInstanceId": 154560,
     "sourceId": 181353,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ml-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 803.741372,
   "end_time": "2024-10-27T06:12:38.713054",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-27T05:59:14.971682",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3ae3375ceefd42f9aeede66494e08172": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_712906b89a034aa981c78ec917695c16",
        "IPY_MODEL_6e02449d72574cb59a7408135ddf0606",
        "IPY_MODEL_aabe4ff8b2734c9c86a33deb7a3d0a52"
       ],
       "layout": "IPY_MODEL_d865f8a6c163451da7719a6d6e720f96"
      }
     },
     "63273838b2cf407a9d8a4b3d3c0a6096": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64efbe72d2c24a98b72db29e311e7206": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6e02449d72574cb59a7408135ddf0606": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_805d7686fa99450fa5f28d53a2e50938",
       "max": 11,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c76df798b0e0495abc6131f26e897eca",
       "value": 11
      }
     },
     "712906b89a034aa981c78ec917695c16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af9c9877f8c347788ca34cdd8170fc94",
       "placeholder": "​",
       "style": "IPY_MODEL_c9c855af5c4941d6bed6542ce711a012",
       "value": ""
      }
     },
     "805d7686fa99450fa5f28d53a2e50938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aabe4ff8b2734c9c86a33deb7a3d0a52": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_63273838b2cf407a9d8a4b3d3c0a6096",
       "placeholder": "​",
       "style": "IPY_MODEL_64efbe72d2c24a98b72db29e311e7206",
       "value": "Loading safetensors checkpoint shards: 100% Completed | 11/11 [04:33&lt;00:00, 26.87s/it]\n"
      }
     },
     "af9c9877f8c347788ca34cdd8170fc94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c76df798b0e0495abc6131f26e897eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9c855af5c4941d6bed6542ce711a012": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d865f8a6c163451da7719a6d6e720f96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
